# -*- coding: utf-8 -*-
"""mental_health_bert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jfnEKPkBPGE82rZAkpjwOg_czx5cHa3T

CSE 256 Final Project - Fine-tuning BERT for Sentiment Analysis of Mental Health Dataset
"""

# Commented out IPython magic to ensure Python compatibility.
# This mounts your Google Drive to the Colab VM.
from google.colab import drive
drive.mount('/content/drive')

# TODO: Enter the foldername in your Drive where you have saved this notebook
# e.g. 'CSE156/assignments/PA3/'
FOLDERNAME = 'Colab Notebooks/Final_Project'
assert FOLDERNAME is not None, "[!] Enter the foldername."

# Now that we've mounted your Drive, this ensures that
# the Python interpreter of the Colab VM can load
# python files from within it.
import sys
sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))

# This is later used to use the IMDB reviews
# %cd /content/drive/My\ Drive/$FOLDERNAME/

! pip install -q kaggle
from google.colab import files
files.upload()

! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json
! kaggle datasets download suchintikasarkar/sentiment-analysis-for-mental-health

! unzip "sentiment-analysis-for-mental-health.zip"

from tqdm.notebook import tqdm
import pandas as pd
import os
import csv
import sys
import numpy as np
import time
import random
from typing import Optional, List, Tuple
import matplotlib.pyplot as plt
import textwrap
import torch

seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)

df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Final_Project/sentiment-analysis-for-mental-health.zip',index_col=0)
df.head()

print(df.isnull().values.any())
print(df.isnull().sum())
df = df.dropna()

sentiment_counts=df['status'].value_counts()
print(sentiment_counts)

df.describe()

import seaborn as sns
df.groupby('status').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

sentiment_counts.plot(kind='pie', autopct='%1.1f%%', figsize=(8, 8), title='Sentiment Distribution')
plt.title('Distribution of Mental Health Status')
plt.savefig('sentiment_distribution.png')
plt.show()

"""class imbalance -> try to use a weighted cross-entropy loss function"""

df['statement_length'] = df['statement'].apply(len)
df['num_words'] = df['statement'].apply(lambda x: len(x.split()))
df

df['statement_length'].hist(bins=100)
# df['num_words'].hist(bins=100)
plt.title('Distribution of Statement Lengths')
plt.xlabel('Length of Statements')
plt.ylabel('Frequency')
plt.show()

log_df = df.copy()
log_df['statement_length'] = np.log(log_df['statement_length'])
log_df['num_words'] = np.log(log_df['num_words'])
log_df['statement_length'].hist(bins=100)
plt.title('Distribution of Statement Lengths')
plt.xlabel('Length of Statements (log-transformed)')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize= (8,5))
print(df.groupby('status')["statement_length"].mean())
df.groupby('status')["statement_length"].mean().sort_values().plot()
plt.savefig('statement_length_by_status.png')

plt.figure(figsize= (8,5))
print(df.groupby('status')["num_words"].mean())
df.groupby('status')["num_words"].mean().sort_values().plot()
plt.savefig('num_words_by_status.png')

log_df

"""# Baseline Models: Logistic Regression, SVM, and Random Forest Classifier using TF-IDF metrics"""

from sklearn.model_selection import train_test_split

X = df['statement']  # Feature columns
y = df['status']  # Target column

# Initial split: Train+Validation vs Test (80-20 split)
X_trainval, X_test, y_trainval, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Second split: Train vs Validation (60-20-20 overall split)
X_train, X_val, y_train, y_val = train_test_split(
    X_trainval, y_trainval, test_size=0.25, random_state=42
)

print(f"Training Set Size: {len(X_train)}")
print(f"Validation Set Size: {len(X_val)}")
print(f"Test Set Size: {len(X_test)}")

"""1. Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score

max_features_list = [1000, 2000, 5000]
C_values = [0.01, 0.1, 1, 10]
results = []

for max_features in max_features_list:
    for C in C_values:
        vectorizer = TfidfVectorizer(max_features=max_features)
        X_train_tfidf = vectorizer.fit_transform(X_train)
        X_val_tfidf = vectorizer.transform(X_val)

        lr_model = LogisticRegression(C=C, max_iter=1000, random_state = 42)
        lr_model.fit(X_train_tfidf, y_train)
        y_val_pred = lr_model.predict(X_val_tfidf)

        # Eval on validation set
        val_accuracy = accuracy_score(y_val, y_val_pred)
        results.append((max_features, C, val_accuracy))

        print(f"max_features={max_features}, C={C}, Validation Accuracy={val_accuracy:.4f}")

# get best param to use for test set
best_result = max(results, key=lambda x: x[2])
best_max_features, best_C, best_val_accuracy = best_result
print("\nBest parameters based on validation set:")
print(f"max_features={best_max_features}, C={best_C}, Validation Accuracy={best_val_accuracy:.4f}")

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_features=5000)
X_trainval_tfidf = vectorizer.fit_transform(X_trainval)
X_test_tfidf = vectorizer.transform(X_test)

from sklearn.linear_model import LogisticRegression

# Best model
lr_model = LogisticRegression(C=1, max_iter=1000)
lr_model.fit(X_trainval_tfidf, y_trainval)
y_pred = lr_model.predict(X_test_tfidf)

misclassified_indices = [i for i, (true, pred) in enumerate(zip(y_test, y_pred)) if true != pred]
pd.reset_option('display.max_colwidth')

# see what logistic regression model misclassified on
misclassified_df = pd.DataFrame({
    'True Label': [y_test.iloc[i] for i in misclassified_indices],
    'Predicted Label': [y_pred[i] for i in misclassified_indices],
    'Statement': [X_test.iloc[i] for i in misclassified_indices]
})

print("Misclassified Samples:")
print(misclassified_df.head(10))

misclassified_df.to_csv("misclassified_samples.csv", index=False)

target_names = df['status'].unique()
report = classification_report(y_test, y_pred, target_names=target_names)
print("Logistic Regression Model Performance:")
print(report)

# Calculate accuracy for test set
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=target_names)
plt.show()

"""2. Support Vector Machine Model"""

from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score

max_features_list = [1000, 2000, 5000]
C_values = [0.01, 0.1, 1, 10]  # Regularization parameter C

results = []

for max_features in max_features_list:
    for C in C_values:
        vectorizer = TfidfVectorizer(max_features=max_features)
        X_train_tfidf = vectorizer.fit_transform(X_train)
        X_val_tfidf = vectorizer.transform(X_val)

        # Train SVM with current C value
        svm_model = LinearSVC(C=C, max_iter=1000)
        svm_model.fit(X_train_tfidf, y_train)
        y_val_pred = svm_model.predict(X_val_tfidf)

        val_accuracy = accuracy_score(y_val, y_val_pred)
        results.append((max_features, C, val_accuracy))

        print(f"max_features={max_features}, C={C}, Accuracy={val_accuracy:.4f}")

best_result = max(results, key=lambda x: x[2])
print("\nBest parameters:")
print(f"max_features={best_result[0]}, C={best_result[1]}, Accuracy={best_result[2]:.4f}")

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_features=5000)
X_trainval_tfidf = vectorizer.fit_transform(X_trainval)
X_test_tfidf = vectorizer.transform(X_test)

from sklearn.svm import LinearSVC

# Best model
svm_model = LinearSVC(C=1, max_iter=1000)
svm_model.fit(X_trainval_tfidf, y_trainval)
y_pred = svm_model.predict(X_test_tfidf)

misclassified_indices = [i for i, (true, pred) in enumerate(zip(y_test, y_pred)) if true != pred]
pd.reset_option('display.max_colwidth')

misclassified_df = pd.DataFrame({
    'True Label': [y_test.iloc[i] for i in misclassified_indices],
    'Predicted Label': [y_pred[i] for i in misclassified_indices],
    'Statement': [X_test.iloc[i] for i in misclassified_indices]
})

print("Misclassified Samples:")
print(misclassified_df.head(10))

misclassified_df.to_csv("svm_misclassified_samples.csv", index=False)

report = classification_report(y_test, y_pred, target_names=target_names)
print("SVM Model Performance:")
print(report)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=target_names)
plt.show()

"""3. Random Forest Classifier"""

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_val_tfidf = vectorizer.transform(X_val)
X_test_tfidf = vectorizer.transform(X_test)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

# Define hyperparameter ranges
n_estimators_list = [50, 100, 200]  # Number of trees in the forest
max_depth_list = [10, 20, 30]  # Max depth of the trees

results = []

for n_estimators in n_estimators_list:
    for max_depth in max_depth_list:
          rf_model = RandomForestClassifier(
              n_estimators=n_estimators,
              max_depth=max_depth,
              random_state=42
          )
          rf_model.fit(X_train_tfidf, y_train)
          y_val_pred = rf_model.predict(X_val_tfidf)

          accuracy = accuracy_score(y_val, y_val_pred)
          results.append((n_estimators, max_depth, accuracy))

          print(f"n_estimators={n_estimators}, max_depth={max_depth}, Accuracy={accuracy:.4f}")

best_result = max(results, key=lambda x: x[2])
print("\nBest hyperparameters:")
print(f"n_estimators={best_result[0]}, max_depth={best_result[1]}, Accuracy={best_result[2]:.4f}")

# Results with max_features = 5000
# n_estimators=50, max_depth=10, Accuracy=0.5570
# n_estimators=50, max_depth=20, Accuracy=0.6345
# n_estimators=50, max_depth=30, Accuracy=0.6734
# n_estimators=100, max_depth=10, Accuracy=0.5556
# n_estimators=100, max_depth=20, Accuracy=0.6364
# n_estimators=100, max_depth=30, Accuracy=0.6797
# n_estimators=200, max_depth=10, Accuracy=0.5525
# n_estimators=200, max_depth=20, Accuracy=0.6357
# n_estimators=200, max_depth=30, Accuracy=0.6788

#Best Model
rf_model = RandomForestClassifier(
              n_estimators=200,
              max_depth=30,
              random_state=42
          )
rf_model.fit(X_train_tfidf, y_train)
y_pred = rf_model.predict(X_test_tfidf)

target_names = df['status'].unique()

report = classification_report(y_test, y_pred, target_names = target_names)
print("Random Forest Model Performance:")
print(report)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=target_names)
plt.show()

"""# My Approach: Utilizing pretrained language models (BERT) for sentiment analysis"""

from transformers import AutoTokenizer
def preprocess_statements(statements, tokenizer, max_length=128):

    # Tokenize with padding and truncation
    encoding = tokenizer(
        statements,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )
    return encoding['input_ids'], encoding['attention_mask']

local_path = '/content/drive/MyDrive/Colab Notebooks/Final_Project/mental_health_bert_model'
# hf_path = 'bert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(local_path)

input_ids, attention_mask = preprocess_statements(df['statement'].tolist(), tokenizer)

label_mapping = {label: i for i, label in enumerate(df['status'].unique())}
labels = df['status'].map(label_mapping).tolist()

labels = torch.tensor(labels)

print(label_mapping)

class MentalHealthDataset(torch.utils.data.Dataset):
    def __init__(self, input_ids, attention_mask, labels):
        self.input_ids = input_ids
        self.attention_mask = attention_mask
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            'input_ids': self.input_ids[idx],
            'attention_mask': self.attention_mask[idx],
            'labels': self.labels[idx]
        }

dataset = MentalHealthDataset(input_ids, attention_mask, labels)

from sklearn.model_selection import train_test_split

train_idx, val_idx = train_test_split(range(len(dataset)), test_size=0.2, stratify=labels, random_state = 42)

train_dataset = torch.utils.data.Subset(dataset, train_idx)
val_dataset = torch.utils.data.Subset(dataset, val_idx)

# Create dataloaders
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)

import torch
from transformers import AutoModelForSequenceClassification, AdamW
from torch.nn import CrossEntropyLoss
from torch.utils.data import DataLoader
from transformers import get_scheduler
from tqdm import tqdm

num_labels = 7
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=num_labels)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

optimizer = AdamW(model.parameters(), lr=5e-5)

num_epochs = 3
num_training_steps = len(train_loader) * num_epochs
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps
)
criterion = CrossEntropyLoss()

def train_model(model, train_loader, val_loader, optimizer, scheduler, device, epochs=3):
    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}/{epochs}")
        print("-" * 30)

        model.train()
        train_loss = 0
        correct = 0
        total = 0

        for batch in tqdm(train_loader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            loss = criterion(logits, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            scheduler.step()

            train_loss += loss.item()
            _, preds = torch.max(logits, dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

        train_acc = correct / total
        avg_train_loss = train_loss / len(train_loader)
        print(f"Training Loss: {avg_train_loss:.4f}, Accuracy: {train_acc:.4f}")

        model.eval()
        val_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)

                outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outputs.logits
                loss = criterion(logits, labels)

                val_loss += loss.item()
                _, preds = torch.max(logits, dim=1)
                correct += (preds == labels).sum().item()
                total += labels.size(0)

        val_acc = correct / total
        avg_val_loss = val_loss / len(val_loader)
        print(f"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_acc:.4f}")


train_model(model, train_loader, val_loader, optimizer, lr_scheduler, device, epochs=num_epochs)

model.save_pretrained("/content/drive/My Drive/mental_health_bert_model")
tokenizer.save_pretrained("/content/drive/My Drive/mental_health_bert_model")

label_mapping_inv = {
    0: 'Anxiety',
    1: 'Normal',
    2: 'Depression',
    3: 'Suicidal',
    4: 'Stress',
    5: 'Bipolar',
    6: 'Personality disorder'
}

from sklearn.metrics import classification_report, accuracy_score

def evaluate_model(model, val_loader, device):
    model.eval()

    all_preds = []
    all_labels = []
    all_examples = []

    with torch.no_grad():
        for batch_idx, batch in enumerate(val_loader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            preds = torch.argmax(logits, dim=1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

            batch_indices = val_loader.dataset.indices[batch_idx * len(batch):(batch_idx + 1) * len(batch)]
            for i, idx in enumerate(batch_indices):
                statement = df['statement'].iloc[idx]
                all_examples.append({
                    'index': idx,
                    'statement': statement,
                    'true_label': label_mapping_inv[labels[i].item()],
                    'predicted_label': label_mapping_inv[preds[i].item()]
                })

    all_df = pd.DataFrame(all_examples)

    all_df.to_csv('bert_all_examples.csv', index=False)

    print("Evaluation Metrics:")
    print(classification_report(all_labels, all_preds, target_names=label_mapping.keys()))
    print(f"Accuracy: {accuracy_score(all_labels, all_preds):.4f}")

    return all_df, all_preds, all_labels


all_df, all_preds, all_labels = evaluate_model(model, val_loader, device)
print(all_df.head())

from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_predictions(all_labels, all_preds, display_labels=label_mapping.keys())
plt.show()

class_frequencies = [3841, 16343, 15404, 10652, 2777, 2587, 1077]
class_weights = torch.tensor([1 / freq for freq in class_frequencies]).to(device)
criterion = CrossEntropyLoss(weight=class_weights)

num_epochs = 5
num_training_steps = len(train_loader) * num_epochs
lr_scheduler = get_scheduler(
    "cosine",
    optimizer=optimizer,
    num_warmup_steps=100,
    num_training_steps=num_training_steps
)



def train_model(model, train_loader, val_loader, optimizer, scheduler, criterion, device, epochs=3):
    start_epoch = 0
    if os.path.isfile(checkpoint_path):
        start_epoch = load_checkpoint(model, optimizer, scheduler, checkpoint_path)

    for epoch in range(start_epoch, epochs):
        print(f"Epoch {epoch + 1}/{epochs}")
        print("-" * 30)

        model.train()
        train_loss = 0
        correct = 0
        total = 0

        for batch in tqdm(train_loader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            loss = criterion(logits, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            scheduler.step()

            train_loss += loss.item()
            _, preds = torch.max(logits, dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

        train_acc = correct / total
        avg_train_loss = train_loss / len(train_loader)
        print(f"Training Loss: {avg_train_loss:.4f}, Accuracy: {train_acc:.4f}")

        model.eval()
        val_loss = 0
        y_true = []
        y_pred = []

        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)

                outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outputs.logits
                loss = criterion(logits, labels)

                val_loss += loss.item()
                _, preds = torch.max(logits, dim=1)

                y_true.extend(labels.cpu().numpy())
                y_pred.extend(preds.cpu().numpy())

        avg_val_loss = val_loss / len(val_loader)
        print(f"Validation Loss: {avg_val_loss:.4f}")

        report = classification_report(y_true, y_pred, target_names=["Anxiety", "Normal", "Depression", "Suicidal", "Stress", "Bipolar", "Personality Disorder"])
        print(report)



train_model(model, train_loader, val_loader, optimizer, lr_scheduler, criterion, device, epochs=num_epochs)

all_preds, all_labels = evaluate_model(model, val_loader, device)

ConfusionMatrixDisplay.from_predictions(all_labels, all_preds, display_labels=label_mapping.keys())
plt.show()